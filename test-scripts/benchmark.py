#!/usr/bin/env python3
"""
Git Merge Orchestrator - æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒè§„æ¨¡å’Œåœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨çŽ°
"""

import os
import sys
import json
import subprocess
import time
import statistics
from pathlib import Path
from datetime import datetime
from create_test_repo import TestRepoCreator


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self, test_base_dir):
        self.test_base_dir = Path(test_base_dir)
        self.gmo_path = self.test_base_dir.parent / "git-merge-orchestrator"
        self.creator = TestRepoCreator(str(test_base_dir))
        self.benchmark_results = {}
        
    def run_benchmark_suite(self, scenarios=None, iterations=3):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶"""
        if scenarios is None:
            scenarios = ["simple", "complex", "large-scale"]
            
        print("ðŸš€ å¼€å§‹Git Merge Orchestratoræ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"ðŸ”„ æ¯ä¸ªåœºæ™¯è¿è¡Œ {iterations} æ¬¡è¿­ä»£")
        print("=" * 60)
        
        for scenario in scenarios:
            print(f"\nðŸ“Š åŸºå‡†æµ‹è¯•åœºæ™¯: {scenario}")
            print("-" * 40)
            
            try:
                results = self._benchmark_scenario(scenario, iterations)
                self.benchmark_results[scenario] = results
                self._print_scenario_summary(scenario, results)
                
            except Exception as e:
                print(f"âŒ åœºæ™¯ {scenario} åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
                
        self._generate_benchmark_report()

    def _benchmark_scenario(self, scenario, iterations):
        """å¯¹å•ä¸ªåœºæ™¯è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        scenario_configs = {
            "simple": {
                "type": "simple",
                "files": 20,
                "contributors": ["Dev1", "Dev2"],
                "branches": ["feature"]
            },
            "complex": {
                "type": "complex", 
                "files": 100,
                "contributors": ["Dev1", "Dev2", "Dev3", "Dev4", "Dev5"],
                "branches": ["feature-1", "feature-2"]
            },
            "large-scale": {
                "type": "large-scale",
                "files": 500,
                "contributors": [f"Dev{i}" for i in range(1, 11)],
                "branches": ["feature-main"]
            }
        }
        
        if scenario not in scenario_configs:
            raise ValueError(f"æœªçŸ¥åœºæ™¯: {scenario}")
            
        config = scenario_configs[scenario]
        repo_name = f"benchmark-{scenario}"
        
        # åˆ›å»ºæµ‹è¯•ä»“åº“
        repo_path = self._create_benchmark_repo(repo_name, config)
        if not repo_path:
            raise RuntimeError(f"åˆ›å»ºåŸºå‡†æµ‹è¯•ä»“åº“å¤±è´¥: {repo_name}")
            
        # è¿›è¡Œå¤šæ¬¡è¿­ä»£æµ‹è¯•
        iteration_results = []
        
        for i in range(iterations):
            print(f"  ðŸ”„ è¿­ä»£ {i+1}/{iterations}")
            result = self._run_single_benchmark(repo_path, scenario)
            iteration_results.append(result)
            
            # æ¸…ç†å·¥ä½œç›®å½•ä»¥ç¡®ä¿ä¸€è‡´çš„æµ‹è¯•çŽ¯å¢ƒ
            self._cleanup_work_dir(repo_path)
            
        return self._analyze_iteration_results(iteration_results, config)

    def _create_benchmark_repo(self, repo_name, config):
        """åˆ›å»ºåŸºå‡†æµ‹è¯•ä»“åº“"""
        # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§ä»“åº“
        old_repo_path = self.test_base_dir / "test-repos" / repo_name
        if old_repo_path.exists():
            import shutil
            shutil.rmtree(old_repo_path)
            
        # åˆ›å»ºæ–°çš„æµ‹è¯•ä»“åº“
        repo_path = self.creator.create_repo(
            repo_name,
            config["type"],
            contributors=config["contributors"],
            files=config["files"],
            branches=config["branches"]
        )
        
        return repo_path

    def _run_single_benchmark(self, repo_path, scenario):
        """è¿è¡Œå•æ¬¡åŸºå‡†æµ‹è¯•"""
        os.chdir(repo_path)
        
        # æµ‹è¯•ä¸åŒçš„å¤„ç†æ¨¡å¼
        modes = ["file_level", "group_based"]
        mode_results = {}
        
        for mode in modes:
            print(f"    ðŸ“‹ æµ‹è¯•æ¨¡å¼: {mode}")
            
            # ç¡®å®šè¦æµ‹è¯•çš„åˆ†æ”¯
            branches = self._get_test_branches(repo_path)
            if not branches:
                continue
                
            source_branch, target_branch = branches[0], "master"
            
            # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            start_time = time.time()
            
            cmd = [
                "python", str(self.gmo_path / "main.py"),
                "--processing-mode", mode,
                source_branch, target_branch,
                "--auto-analyze", "--quiet"
            ]
            
            try:
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True, 
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                
                end_time = time.time()
                duration = end_time - start_time
                
                # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                metrics = self._collect_performance_metrics(repo_path, duration, result)
                mode_results[mode] = metrics
                
                print(f"      â±ï¸ è€—æ—¶: {duration:.2f}ç§’")
                
            except subprocess.TimeoutExpired:
                print(f"      âš ï¸ è¶…æ—¶ (>300ç§’)")
                mode_results[mode] = {"timeout": True, "duration": 300}
                
            except Exception as e:
                print(f"      âŒ é”™è¯¯: {e}")
                mode_results[mode] = {"error": str(e)}
                
            # æ¸…ç†å·¥ä½œç›®å½•
            self._cleanup_work_dir(repo_path)
            
        return mode_results

    def _get_test_branches(self, repo_path):
        """èŽ·å–å¯ç”¨çš„æµ‹è¯•åˆ†æ”¯"""
        try:
            result = subprocess.run(
                ["git", "branch", "-r"],
                cwd=repo_path,
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                branches = []
                for line in result.stdout.strip().split('\n'):
                    branch = line.strip().replace('origin/', '')
                    if branch and branch != 'master' and not branch.startswith('HEAD'):
                        branches.append(branch)
                return branches
                
        except Exception:
            pass
            
        return ["feature"]  # é»˜è®¤åˆ†æ”¯

    def _collect_performance_metrics(self, repo_path, duration, result):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "duration": duration,
            "success": result.returncode == 0,
            "memory_usage": "N/A"  # å¯ä»¥æ‰©å±•æ·»åŠ å†…å­˜ç›‘æŽ§
        }
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        merge_work_dir = repo_path / ".merge_work"
        if merge_work_dir.exists():
            plan_file = merge_work_dir / "merge_plan.json"
            if plan_file.exists():
                try:
                    with open(plan_file) as f:
                        plan_data = json.load(f)
                    
                    # åˆ†æžå¤„ç†çš„æ•°æ®é‡
                    if plan_data.get("processing_mode") == "file_level":
                        metrics["processed_files"] = len(plan_data.get("files", []))
                        metrics["processing_mode"] = "file_level"
                    else:
                        groups = plan_data.get("groups", [])
                        metrics["processed_groups"] = len(groups)
                        metrics["processed_files"] = sum(len(g.get("files", [])) for g in groups)
                        metrics["processing_mode"] = "group_based"
                        
                    metrics["contributors_analyzed"] = len(plan_data.get("contributors", {}))
                    
                except json.JSONDecodeError:
                    pass
        
        # åˆ†æžè„šæœ¬ç”Ÿæˆæƒ…å†µ
        scripts_dir = merge_work_dir / "scripts" if merge_work_dir.exists() else None
        if scripts_dir and scripts_dir.exists():
            script_count = len(list(scripts_dir.glob("*.sh")))
            metrics["generated_scripts"] = script_count
            
        return metrics

    def _cleanup_work_dir(self, repo_path):
        """æ¸…ç†å·¥ä½œç›®å½•"""
        merge_work_dir = repo_path / ".merge_work"
        if merge_work_dir.exists():
            import shutil
            shutil.rmtree(merge_work_dir)

    def _analyze_iteration_results(self, iteration_results, config):
        """åˆ†æžè¿­ä»£ç»“æžœ"""
        analysis = {
            "config": config,
            "iterations": len(iteration_results),
            "modes": {}
        }
        
        # åˆ†æžæ¯ç§æ¨¡å¼çš„ç»“æžœ
        for mode in ["file_level", "group_based"]:
            mode_data = []
            
            for iteration in iteration_results:
                if mode in iteration and "duration" in iteration[mode]:
                    mode_data.append(iteration[mode])
            
            if mode_data:
                durations = [d["duration"] for d in mode_data if "duration" in d]
                
                if durations:
                    analysis["modes"][mode] = {
                        "avg_duration": statistics.mean(durations),
                        "min_duration": min(durations),
                        "max_duration": max(durations),
                        "std_deviation": statistics.stdev(durations) if len(durations) > 1 else 0,
                        "success_rate": sum(1 for d in mode_data if d.get("success", False)) / len(mode_data),
                        "sample_metrics": mode_data[0]  # ä½¿ç”¨ç¬¬ä¸€æ¬¡è¿­ä»£çš„è¯¦ç»†æŒ‡æ ‡
                    }
        
        return analysis

    def _print_scenario_summary(self, scenario, results):
        """æ‰“å°åœºæ™¯æ‘˜è¦"""
        print(f"\nðŸ“Š åœºæ™¯ {scenario} åŸºå‡†æµ‹è¯•ç»“æžœ:")
        
        for mode, data in results["modes"].items():
            print(f"\n  ðŸ”§ {mode} æ¨¡å¼:")
            print(f"    â±ï¸ å¹³å‡è€—æ—¶: {data['avg_duration']:.2f}ç§’")
            print(f"    ðŸ“Š è€—æ—¶èŒƒå›´: {data['min_duration']:.2f}s - {data['max_duration']:.2f}s")
            print(f"    ðŸ“ˆ æ ‡å‡†å·®: {data['std_deviation']:.2f}ç§’")
            print(f"    âœ… æˆåŠŸçŽ‡: {data['success_rate']*100:.1f}%")
            
            sample = data["sample_metrics"]
            if "processed_files" in sample:
                print(f"    ðŸ“ å¤„ç†æ–‡ä»¶æ•°: {sample['processed_files']}")
            if "contributors_analyzed" in sample:
                print(f"    ðŸ‘¥ åˆ†æžè´¡çŒ®è€…: {sample['contributors_analyzed']}")

    def _generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": self._generate_summary(),
            "scenarios": self.benchmark_results,
            "system_info": self._collect_system_info()
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.test_base_dir / "logs" / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦æŠ¥å‘Š
        self._print_benchmark_summary(report)
        
        print(f"\nðŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report

    def _generate_summary(self):
        """ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡"""
        summary = {
            "total_scenarios": len(self.benchmark_results),
            "mode_comparison": {},
            "performance_grades": {}
        }
        
        # æ¯”è¾ƒä¸åŒæ¨¡å¼çš„æ€§èƒ½
        for mode in ["file_level", "group_based"]:
            mode_durations = []
            for scenario_results in self.benchmark_results.values():
                if mode in scenario_results["modes"]:
                    mode_durations.append(scenario_results["modes"][mode]["avg_duration"])
            
            if mode_durations:
                summary["mode_comparison"][mode] = {
                    "avg_duration": statistics.mean(mode_durations),
                    "scenarios_tested": len(mode_durations)
                }
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        for scenario, results in self.benchmark_results.items():
            grades = {}
            for mode, data in results["modes"].items():
                avg_time = data["avg_duration"]
                success_rate = data["success_rate"]
                
                # åŸºäºŽæ—¶é—´å’ŒæˆåŠŸçŽ‡çš„ç®€å•è¯„çº§
                if success_rate >= 0.9:
                    if avg_time <= 10:
                        grade = "ä¼˜ç§€"
                    elif avg_time <= 30:
                        grade = "è‰¯å¥½"
                    elif avg_time <= 60:
                        grade = "ä¸€èˆ¬"
                    else:
                        grade = "éœ€ä¼˜åŒ–"
                else:
                    grade = "ä¸ç¨³å®š"
                    
                grades[mode] = grade
                
            summary["performance_grades"][scenario] = grades
        
        return summary

    def _collect_system_info(self):
        """æ”¶é›†ç³»ç»Ÿä¿¡æ¯"""
        try:
            import platform
            import psutil
            
            return {
                "platform": platform.platform(),
                "python_version": platform.python_version(),
                "cpu_count": psutil.cpu_count(),
                "memory_total": f"{psutil.virtual_memory().total / (1024**3):.1f} GB",
                "git_version": self._get_git_version()
            }
        except ImportError:
            return {
                "platform": "æœªçŸ¥",
                "note": "éœ€è¦å®‰è£… psutil åº“èŽ·å–è¯¦ç»†ç³»ç»Ÿä¿¡æ¯"
            }

    def _get_git_version(self):
        """èŽ·å–Gitç‰ˆæœ¬"""
        try:
            result = subprocess.run(
                ["git", "--version"],
                capture_output=True,
                text=True
            )
            return result.stdout.strip() if result.returncode == 0 else "æœªçŸ¥"
        except Exception:
            return "æœªçŸ¥"

    def _print_benchmark_summary(self, report):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ðŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æ‘˜è¦æŠ¥å‘Š")
        print("=" * 60)
        
        summary = report["summary"]
        
        print(f"æµ‹è¯•åœºæ™¯æ•°: {summary['total_scenarios']}")
        print(f"æµ‹è¯•æ—¶é—´: {report['timestamp'][:19]}")
        
        # æ¨¡å¼å¯¹æ¯”
        if summary.get("mode_comparison"):
            print("\nðŸ”§ å¤„ç†æ¨¡å¼æ€§èƒ½å¯¹æ¯”:")
            for mode, data in summary["mode_comparison"].items():
                print(f"  {mode}: å¹³å‡ {data['avg_duration']:.2f}ç§’ ({data['scenarios_tested']} ä¸ªåœºæ™¯)")
        
        # æ€§èƒ½ç­‰çº§
        print("\nðŸ† æ€§èƒ½ç­‰çº§è¯„ä¼°:")
        for scenario, grades in summary["performance_grades"].items():
            print(f"  ðŸ“‹ {scenario}:")
            for mode, grade in grades.items():
                print(f"    {mode}: {grade}")
        
        # ç³»ç»Ÿä¿¡æ¯
        if report.get("system_info"):
            print(f"\nðŸ’» æµ‹è¯•çŽ¯å¢ƒ: {report['system_info'].get('platform', 'æœªçŸ¥')}")
            if "cpu_count" in report["system_info"]:
                print(f"    CPU: {report['system_info']['cpu_count']} æ ¸å¿ƒ")
                print(f"    å†…å­˜: {report['system_info']['memory_total']}")

        print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Git Merge Orchestrator æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument(
        "--scenarios",
        default="simple,complex,large-scale",
        help="æµ‹è¯•åœºæ™¯ï¼Œé€—å·åˆ†éš”"
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=3,
        help="æ¯ä¸ªåœºæ™¯çš„è¿­ä»£æ¬¡æ•°"
    )
    parser.add_argument(
        "--output",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
    )
    parser.add_argument(
        "--test-dir",
        default="/home/howie/Workspace/Project/tools/git-merge-orchestrator-test",
        help="æµ‹è¯•ç›®å½•è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # è§£æžåœºæ™¯åˆ—è¡¨
    scenarios = [s.strip() for s in args.scenarios.split(",")]
    
    # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
    original_dir = os.getcwd()
    
    try:
        benchmark = PerformanceBenchmark(args.test_dir)
        benchmark.run_benchmark_suite(scenarios, args.iterations)
        
        # å¦‚æžœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œå¤åˆ¶æŠ¥å‘Š
        if args.output:
            import shutil
            # æ‰¾åˆ°æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
            logs_dir = Path(args.test_dir) / "logs"
            if logs_dir.exists():
                report_files = list(logs_dir.glob("benchmark_report_*.json"))
                if report_files:
                    latest_report = max(report_files, key=os.path.getctime)
                    shutil.copy2(latest_report, args.output)
                    print(f"ðŸ“‹ æŠ¥å‘Šå·²å¤åˆ¶åˆ°: {args.output}")
        
    finally:
        os.chdir(original_dir)


if __name__ == "__main__":
    main()